{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582c8069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Downloading pytorch_model.bin:   1%|       | 41.9M/5.68G [00:19<15:26, 6.09MB/s]"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8bd4404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading (…)lve/main/config.json: 100%|██████| 819/819 [00:00<00:00, 137kB/s]\u001b[A\n",
      "\n",
      "Downloading (…)instruct_pipeline.py: 100%|█| 9.16k/9.16k [00:00<00:00, 7.30MB/s]\u001b[A\n",
      "A new version of the following files was downloaded from https://huggingface.co/databricks/dolly-v2-7b:\n",
      "- instruct_pipeline.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "\n",
      "Downloading pytorch_model.bin:   0%|                | 0.00/13.8G [00:00<?, ?B/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   0%|       | 10.5M/13.8G [00:01<37:06, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   0%|       | 21.0M/13.8G [00:03<37:39, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   0%|       | 31.5M/13.8G [00:05<38:17, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   0%|       | 41.9M/13.8G [00:06<37:35, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   0%|       | 52.4M/13.8G [00:08<37:17, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   0%|       | 62.9M/13.8G [00:10<37:04, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|       | 73.4M/13.8G [00:11<37:35, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|       | 83.9M/13.8G [00:13<37:30, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|       | 94.4M/13.8G [00:15<39:39, 5.78MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|        | 105M/13.8G [00:17<38:48, 5.90MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|        | 115M/13.8G [00:19<38:04, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|        | 126M/13.8G [00:22<50:09, 4.56MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|        | 136M/13.8G [00:25<56:09, 4.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|        | 147M/13.8G [00:28<53:38, 4.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|        | 157M/13.8G [00:29<48:15, 4.73MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|        | 168M/13.8G [00:31<44:29, 5.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|        | 178M/13.8G [00:33<42:09, 5.40MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|        | 189M/13.8G [00:34<40:14, 5.66MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|        | 199M/13.8G [00:36<40:30, 5.62MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|        | 210M/13.8G [00:38<39:22, 5.77MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 220M/13.8G [00:40<38:39, 5.87MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 231M/13.8G [00:41<37:50, 6.00MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 241M/13.8G [00:43<37:17, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 252M/13.8G [00:45<36:55, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 262M/13.8G [00:46<36:46, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 273M/13.8G [00:48<37:38, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 283M/13.8G [00:50<37:02, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 294M/13.8G [00:51<36:49, 6.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 304M/13.8G [00:53<36:30, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 315M/13.8G [00:55<36:24, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 325M/13.8G [00:56<36:21, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 336M/13.8G [00:58<36:12, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 346M/13.8G [01:00<36:06, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▏       | 357M/13.8G [01:01<36:10, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▏       | 367M/13.8G [01:03<36:14, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▏       | 377M/13.8G [01:05<37:10, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▏       | 388M/13.8G [01:07<36:48, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▏       | 398M/13.8G [01:08<36:23, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▏       | 409M/13.8G [01:10<36:16, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▏       | 419M/13.8G [01:12<36:06, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▏       | 430M/13.8G [01:13<36:12, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▎       | 440M/13.8G [01:15<36:05, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▎       | 451M/13.8G [01:17<35:57, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▎       | 461M/13.8G [01:19<35:58, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▎       | 472M/13.8G [01:20<35:42, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▎       | 482M/13.8G [01:22<36:48, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 493M/13.8G [01:24<36:30, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 503M/13.8G [01:25<35:55, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 514M/13.8G [01:27<35:54, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 524M/13.8G [01:29<35:34, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 535M/13.8G [01:30<35:29, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 545M/13.8G [01:32<36:38, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 556M/13.8G [01:34<36:24, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 566M/13.8G [01:36<35:53, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 577M/13.8G [01:37<35:50, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 587M/13.8G [01:39<35:53, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 598M/13.8G [01:41<36:49, 6.00MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 608M/13.8G [01:43<36:36, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 619M/13.8G [01:44<36:04, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▎       | 629M/13.8G [01:46<36:01, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▎       | 640M/13.8G [01:48<35:38, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 650M/13.8G [01:49<35:12, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 661M/13.8G [01:51<35:19, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 671M/13.8G [01:53<36:23, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 682M/13.8G [01:54<35:57, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 692M/13.8G [01:56<35:38, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 703M/13.8G [01:58<36:00, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 713M/13.8G [02:00<35:18, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 724M/13.8G [02:01<36:22, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 734M/13.8G [02:03<35:59, 6.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 744M/13.8G [02:05<35:30, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 755M/13.8G [02:06<35:22, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 765M/13.8G [02:08<35:04, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 776M/13.8G [02:10<35:05, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 786M/13.8G [02:12<37:09, 5.86MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 797M/13.8G [02:14<36:38, 5.93MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 807M/13.8G [02:16<41:52, 5.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 818M/13.8G [02:18<42:01, 5.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 828M/13.8G [02:20<41:37, 5.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 839M/13.8G [02:22<43:14, 5.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 849M/13.8G [02:24<42:33, 5.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 860M/13.8G [02:27<43:25, 4.98MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▌       | 870M/13.8G [02:29<42:50, 5.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▌       | 881M/13.8G [02:31<41:59, 5.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▌       | 891M/13.8G [02:33<42:59, 5.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 902M/13.8G [02:34<40:40, 5.30MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 912M/13.8G [02:36<39:01, 5.52MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 923M/13.8G [02:38<37:21, 5.77MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 933M/13.8G [02:40<36:33, 5.89MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 944M/13.8G [02:41<37:06, 5.79MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 954M/13.8G [02:43<36:11, 5.94MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:   7%|▌       | 965M/13.8G [02:45<35:34, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 975M/13.8G [02:46<35:17, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 986M/13.8G [02:48<34:57, 6.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 996M/13.8G [02:50<34:32, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌      | 1.01G/13.8G [02:51<34:31, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌      | 1.02G/13.8G [02:53<34:16, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌      | 1.03G/13.8G [02:55<34:10, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌      | 1.04G/13.8G [02:57<35:13, 6.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.05G/13.8G [02:58<35:15, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.06G/13.8G [03:00<35:11, 6.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.07G/13.8G [03:02<36:32, 5.83MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.08G/13.8G [03:04<36:25, 5.84MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.09G/13.8G [03:06<35:59, 5.91MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.10G/13.8G [03:07<35:17, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.11G/13.8G [03:09<35:02, 6.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.12G/13.8G [03:11<35:22, 6.00MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.13G/13.8G [03:13<39:11, 5.41MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.14G/13.8G [03:15<38:38, 5.48MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.15G/13.8G [03:17<37:48, 5.59MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.16G/13.8G [03:18<36:51, 5.73MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌      | 1.17G/13.8G [03:20<35:51, 5.89MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▌      | 1.18G/13.8G [03:22<35:07, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▌      | 1.20G/13.8G [03:24<34:51, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▌      | 1.21G/13.8G [03:26<38:31, 5.47MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▌      | 1.22G/13.8G [03:28<38:45, 5.43MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▌      | 1.23G/13.8G [03:30<41:14, 5.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▋      | 1.24G/13.8G [03:32<38:54, 5.40MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▋      | 1.25G/13.8G [03:34<38:26, 5.46MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▋      | 1.26G/13.8G [03:35<37:35, 5.58MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▋      | 1.27G/13.8G [03:37<37:21, 5.61MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▋      | 1.28G/13.8G [03:39<37:35, 5.57MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▋      | 1.29G/13.8G [03:42<42:16, 4.95MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▋      | 1.30G/13.8G [03:44<41:53, 4.99MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▋      | 1.31G/13.8G [03:46<41:09, 5.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▋      | 1.32G/13.8G [03:49<44:20, 4.71MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▍    | 1.33G/13.8G [03:54<1:01:22, 3.40MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▋      | 1.34G/13.8G [03:56<56:55, 3.66MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▋      | 1.35G/13.8G [03:58<51:36, 4.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▋      | 1.36G/13.8G [04:00<46:35, 4.46MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▋      | 1.37G/13.8G [04:02<43:23, 4.79MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▋      | 1.38G/13.8G [04:04<42:48, 4.85MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▋      | 1.39G/13.8G [04:06<45:10, 4.59MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▋      | 1.41G/13.8G [04:08<43:59, 4.71MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▋      | 1.42G/13.8G [04:10<41:49, 4.95MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▋      | 1.43G/13.8G [04:12<39:26, 5.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▋      | 1.44G/13.8G [04:14<40:49, 5.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▋      | 1.45G/13.8G [04:16<41:03, 5.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▋      | 1.46G/13.8G [04:18<39:46, 5.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▋      | 1.47G/13.8G [04:20<39:48, 5.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▋      | 1.48G/13.8G [04:22<37:47, 5.45MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊      | 1.49G/13.8G [04:24<38:45, 5.31MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊      | 1.50G/13.8G [04:26<37:42, 5.46MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊      | 1.51G/13.8G [04:27<36:17, 5.66MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊      | 1.52G/13.8G [04:29<35:27, 5.79MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊      | 1.53G/13.8G [04:31<36:56, 5.56MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊      | 1.54G/13.8G [04:34<39:26, 5.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊      | 1.55G/13.8G [04:36<40:18, 5.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊      | 1.56G/13.8G [04:38<41:10, 4.97MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊      | 1.57G/13.8G [04:40<39:19, 5.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊      | 1.58G/13.8G [04:41<37:21, 5.47MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.59G/13.8G [04:43<35:42, 5.72MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.60G/13.8G [04:45<34:43, 5.88MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.61G/13.8G [04:46<34:08, 5.97MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.63G/13.8G [04:48<34:42, 5.87MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.64G/13.8G [04:50<33:47, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.65G/13.8G [04:52<34:07, 5.96MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.66G/13.8G [04:53<33:49, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.67G/13.8G [04:55<33:35, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.68G/13.8G [04:57<33:51, 5.99MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.69G/13.8G [04:59<35:05, 5.77MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.70G/13.8G [05:01<34:51, 5.81MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.71G/13.8G [05:02<33:56, 5.96MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.72G/13.8G [05:04<33:36, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▊      | 1.73G/13.8G [05:06<33:13, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.74G/13.8G [05:07<33:06, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.75G/13.8G [05:09<32:53, 6.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.76G/13.8G [05:11<34:06, 5.91MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.77G/13.8G [05:13<33:49, 5.95MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.78G/13.8G [05:15<33:52, 5.94MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.79G/13.8G [05:17<38:07, 5.27MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.80G/13.8G [05:19<36:03, 5.57MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.81G/13.8G [05:21<35:58, 5.58MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.82G/13.8G [05:22<34:50, 5.75MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.84G/13.8G [05:24<34:13, 5.85MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.85G/13.8G [05:26<35:02, 5.71MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.86G/13.8G [05:28<34:04, 5.86MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|▉      | 1.87G/13.8G [05:29<33:17, 6.00MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|▉      | 1.88G/13.8G [05:31<33:15, 6.00MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|▉      | 1.89G/13.8G [05:33<34:04, 5.85MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|▉      | 1.90G/13.8G [05:35<33:26, 5.95MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|▉      | 1.91G/13.8G [05:36<32:47, 6.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|▉      | 1.92G/13.8G [05:38<32:30, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|▉      | 1.93G/13.8G [05:40<32:14, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|▉      | 1.94G/13.8G [05:42<34:08, 5.81MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|▉      | 1.95G/13.8G [05:44<35:24, 5.60MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|▉      | 1.96G/13.8G [05:45<34:08, 5.80MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|▉      | 1.97G/13.8G [05:47<33:19, 5.94MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:  14%|█      | 1.98G/13.8G [05:49<32:51, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|█      | 1.99G/13.8G [05:50<32:26, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|█      | 2.00G/13.8G [05:52<32:06, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.01G/13.8G [05:54<32:52, 6.00MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.02G/13.8G [05:56<32:41, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.03G/13.8G [05:57<31:58, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.04G/13.8G [05:59<34:49, 5.65MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.06G/13.8G [06:03<41:52, 4.69MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.07G/13.8G [06:04<38:48, 5.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.08G/13.8G [06:06<36:38, 5.35MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.09G/13.8G [06:08<34:53, 5.62MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.10G/13.8G [06:09<35:03, 5.59MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.11G/13.8G [06:11<33:44, 5.80MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.12G/13.8G [06:13<34:37, 5.65MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.13G/13.8G [06:15<33:40, 5.80MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█      | 2.14G/13.8G [06:17<35:34, 5.49MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█      | 2.15G/13.8G [06:20<41:22, 4.71MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█      | 2.16G/13.8G [06:22<38:24, 5.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█      | 2.17G/13.8G [06:24<38:46, 5.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█      | 2.18G/13.8G [06:25<36:57, 5.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█      | 2.19G/13.8G [06:27<35:16, 5.51MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█      | 2.20G/13.8G [06:29<35:46, 5.43MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█      | 2.21G/13.8G [06:31<34:15, 5.66MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█      | 2.22G/13.8G [06:33<33:45, 5.74MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█▏     | 2.23G/13.8G [06:35<35:30, 5.45MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█▏     | 2.24G/13.8G [06:36<34:07, 5.67MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█▏     | 2.25G/13.8G [06:38<34:23, 5.62MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█▏     | 2.26G/13.8G [06:40<33:33, 5.75MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█▏     | 2.28G/13.8G [06:42<32:40, 5.90MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.29G/13.8G [06:43<31:57, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.30G/13.8G [06:45<31:38, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.31G/13.8G [06:47<31:18, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.32G/13.8G [06:48<31:18, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.33G/13.8G [06:50<32:50, 5.85MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.34G/13.8G [06:52<32:15, 5.95MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.35G/13.8G [06:54<32:36, 5.88MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.36G/13.8G [06:56<31:57, 5.99MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.37G/13.8G [06:57<31:29, 6.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.38G/13.8G [06:59<31:27, 6.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.39G/13.8G [07:01<31:13, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.40G/13.8G [07:02<30:43, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.41G/13.8G [07:05<33:18, 5.72MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▏     | 2.42G/13.8G [07:06<32:31, 5.85MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▏     | 2.43G/13.8G [07:08<33:35, 5.66MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▏     | 2.44G/13.8G [07:10<32:38, 5.82MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▏     | 2.45G/13.8G [07:12<31:52, 5.96MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▏     | 2.46G/13.8G [07:13<32:20, 5.87MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▎     | 2.47G/13.8G [07:15<31:36, 6.00MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▎     | 2.49G/13.8G [07:17<33:27, 5.66MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▎     | 2.50G/13.8G [07:19<32:33, 5.81MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▎     | 2.51G/13.8G [07:21<35:25, 5.33MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▎     | 2.52G/13.8G [07:23<33:59, 5.56MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▎     | 2.53G/13.8G [07:25<33:23, 5.65MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▎     | 2.54G/13.8G [07:27<34:45, 5.42MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▎     | 2.55G/13.8G [07:29<33:40, 5.59MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▎     | 2.56G/13.8G [07:30<32:30, 5.79MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.57G/13.8G [07:32<33:10, 5.67MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.58G/13.8G [07:35<38:02, 4.94MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.59G/13.8G [07:37<36:18, 5.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.60G/13.8G [07:38<34:16, 5.47MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.61G/13.8G [07:40<33:26, 5.60MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.62G/13.8G [07:42<32:34, 5.74MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.63G/13.8G [07:44<31:56, 5.85MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.64G/13.8G [07:45<32:23, 5.77MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.65G/13.8G [07:47<31:41, 5.89MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.66G/13.8G [07:49<30:57, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.67G/13.8G [07:50<30:40, 6.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.68G/13.8G [07:52<30:19, 6.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 2.69G/13.8G [07:54<30:09, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▎     | 2.71G/13.8G [07:56<30:51, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▎     | 2.72G/13.8G [07:57<30:30, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 2.73G/13.8G [07:59<30:22, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 2.74G/13.8G [08:01<29:57, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 2.75G/13.8G [08:02<29:44, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 2.76G/13.8G [08:04<29:35, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 2.77G/13.8G [08:06<30:38, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 2.78G/13.8G [08:08<30:07, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 2.79G/13.8G [08:09<29:52, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 2.80G/13.8G [08:11<29:48, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 2.81G/13.8G [08:13<29:34, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 2.82G/13.8G [08:14<29:30, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 2.83G/13.8G [08:16<30:20, 6.05MB/s]"
     ]
    }
   ],
   "source": [
    "generate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7840fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class journallingClass:\n",
    "    def __init__(self, journalType, title, category, dateCreated, mood, journalClass):\n",
    "        self.journalType = journalType\n",
    "        self.journalTitle = title\n",
    "        self.journalCategory = category \n",
    "        self.dateCreated = dateCreated\n",
    "        self.mood = mood\n",
    "        self.journalClass = journalClass\n",
    "    \n",
    "    def getJournalClass(self):\n",
    "        return self.journalClass\n",
    "        \n",
    "        \n",
    "class journalEntryClass:\n",
    "    def __init__(self, userEntry, harmIntent = 0, modelResponse = []):\n",
    "        self.userEntry = userEntry \n",
    "        self.harmIntent = harmIntent\n",
    "        self.modelResponse = modelResponse\n",
    "#         self.safetyResponse = safetyResponse\n",
    "    def printEntry(self):\n",
    "        print(f\"entry: {self.userEntry}, harmful intent: {self.harmIntent}, model response: {self.modelResponse}\")\n",
    "\n",
    "        \n",
    "    def updateHarmIntent(self, harmIntent):\n",
    "        self.harmIntent = harmIntent\n",
    "    \n",
    "    def addModelResponse(self, response):\n",
    "        self.modelResponse.append(response)\n",
    "        \n",
    "class casualJournal(journallingClass):\n",
    "    \n",
    "    def __init__(self, entries = []):\n",
    "        self.entries = []\n",
    "        \n",
    "    def createNewEntry(self, userEntry,  harmIntent = 0, modelResponse = []):\n",
    "        self.entries.append(journalEntryClass(userEntry, harmIntent, modelResponse))\n",
    "        return self.entries[-1]\n",
    "        \n",
    "    def printJournal(self):\n",
    "        for entry in self.entries:\n",
    "            entry.printEntry()\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "class catharticJournal(journallingClass):\n",
    "    def __init__(self, stage = 0, entries = {}):\n",
    "        self.stage = stage # default stage 0\n",
    "        self.entries = entries\n",
    "        \n",
    "    \n",
    "    def createStage(self, journalEntry):\n",
    "        self.entries[f\"stage{self.stage}\"] = journalEntry\n",
    "    \n",
    "    def updateStage(self):\n",
    "        if self.stage >=5:\n",
    "            return \n",
    "        self.stage += 1\n",
    "        \n",
    "    def createNewEntry(self, userEntry,  harmIntent = 0, modelResponse = []):\n",
    "        self.updateStage()\n",
    "        self.entries[f\"stage{self.stage}\"] = journalEntryClass(userEntry, harmIntent, modelResponse)\n",
    "        return self.entries[f\"stage{self.stage}\"]\n",
    "    \n",
    "    def printJournal(self):\n",
    "        for stage in self.entries.keys():\n",
    "            print(stage)\n",
    "            self.entries[stage].printEntry()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab54b7",
   "metadata": {},
   "source": [
    "### Note\n",
    "* what is it is empty \n",
    "* new entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b7e8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {\n",
    "    # je 1 and 2 are done, je3 is new request \n",
    "    \"entry\": [\"journal entry 1\", \"journal entry 2\", \"Jonny's cat died today, it makes me feel sad. Can you provide some empathy\"],\n",
    "    \"harmfulIntent\": [0,0],\n",
    "    \"modelResponse\": [\"model response\", [\"mr 2.1\", \"mr 2.2\"]],\n",
    "    \"type\": 1, # assume 0 cathartic,\n",
    "    \"title\": \"johny's cat died\", \n",
    "    \"userID\": 8940823, \n",
    "    \"date\": \"14/12/2023\", \n",
    "    \"category\": \"cat died\",\n",
    "    \"mood\": \"mood\"\n",
    "    # harm intent \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28c898c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    if response[\"type\"] == 0: # get type check from backend\n",
    "        journalChild = casualJournal()\n",
    "    else:\n",
    "        journalChild = catharticJournal()\n",
    "        \n",
    "    # pull journalType, title, category, dateCreated from backend\n",
    "    journal = journallingClass(response[\"type\"], response[\"title\"], response[\"category\"], response[\"date\"], response[\"mood\"], journalChild)\n",
    "    \n",
    "    # pull journal entries, loop through and load \n",
    "    for entry, harmfulIntent, modelResponse in zip(response[\"entry\"], response[\"harmfulIntent\"], response[\"modelResponse\"]):\n",
    "        journal.getJournalClass().createNewEntry(entry, harmfulIntent, modelResponse)\n",
    "    \n",
    "\n",
    "    print(\"hi\", response[\"entry\"][-1], \"done\")\n",
    "    currentEntry = journal.getJournalClass().createNewEntry(response[\"entry\"][-1])\n",
    "    \n",
    "    currentEntry.printEntry()\n",
    "    harmIntent = checkHarmfulIntent(currentEntry)\n",
    "    \n",
    "    if harmIntent: \n",
    "        modelResponse = buildHarmresponse() \n",
    "        currentEntry.addModelResponse(modelResponse)\n",
    "        return \n",
    "    \n",
    "    modelResponse = generatePrompt(response[\"type\"], currentEntry)\n",
    "    currentEntry.addModelResponse(modelResponse)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc0c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1981bc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = generate_text(\"my cat died today and im sad\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f07a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbebd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkHarmfulIntent(journalEntry):\n",
    "    # implement later\n",
    "    journalEntry.updateHarmIntent(0)a\n",
    "    return False \n",
    "\n",
    "def buildHarmresponse():\n",
    "    response = \"Reach out to a therapist :)\"\n",
    "    return response\n",
    "\n",
    "def generateCatharticResponse(journalEntry):\n",
    "    return \"this is a cathartic response\"\n",
    "\n",
    "def generateCasualResponse(journalEntry):\n",
    "    print(journalEntry.userEntry)\n",
    "    res = generate_text(journalEntry.userEntry)\n",
    "    print(res)\n",
    "    return res[0]\n",
    "\n",
    "def promptSafetyDetector(journalEntry):\n",
    "    return True\n",
    "\n",
    "def generatePrompt(typeJ, journalEntry):\n",
    "    if typeJ == 1:\n",
    "        response = generateCatharticResponse(journalEntry)\n",
    "\n",
    "    else:\n",
    "        response = generateCasualResponse(journalEntry)\n",
    "    \n",
    "    safety_counter = 0\n",
    "    while(safety_counter<5):\n",
    "        safe = promptSafetyDetector(journalEntry)\n",
    "        safety_counter += 1\n",
    "        if safe:\n",
    "            break\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bcdbef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first checkpoint\n",
      "second checkpoint\n",
      "hi je3 done\n",
      "entry: je3, harmful intent: 0, model response: []\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952fecd9",
   "metadata": {},
   "source": [
    "# Pseudocode\n",
    "\n",
    "Main Function:\n",
    "\t# create / look for journal entry class (ask SS)\n",
    "\tjournal = journalParentClass # backend people will send this\n",
    "\tcreateNewJournal(userEntry)\n",
    "\n",
    "\tcheckHarmfulIntent(journalEntry)\n",
    "\n",
    "\tif journalEntry.harmfulIntent: \n",
    "\t\tbuildHarmresponse() # reach out to resources \n",
    "\t\treturn \n",
    "\t\n",
    "\treturn generate_propmt()\n",
    "\t\n",
    "\n",
    "Generate_Prompt():\n",
    "\tif type == Cathartic:\n",
    "\t\tresponse = generateCatharticResponse()\n",
    "\telse:\n",
    "\t\tresponse = generateCasualResponse()\n",
    "\tsafety_counter = 0\n",
    "\twhile(safety_counter<5):\n",
    "\t\tsafe = promptSafetyDetector(journalEntry)\n",
    "\t\tsafety_coutner ++\n",
    "\t\tif safe:\n",
    "\t\t\tbreak\n",
    "\n",
    "\tif safe:\n",
    "\t\treturn journalEntry \n",
    "\n",
    "\t# if not safe\n",
    "\t\n",
    "\treliableQuestionClassifier()\n",
    "\treturn journalEntry\n",
    "\t\n",
    "\n",
    "\n",
    "checkHarmfulIntent(journalEntry):\n",
    "\tif not harm:\n",
    "\t\tjournalEntry.harmfulIntent = False\n",
    "\t\treturn \n",
    "\t\n",
    "\tjournalEntry.harmfulIntent = True\n",
    "\n",
    "\n",
    "\n",
    "generateCasualResponse():\n",
    "\t# generate empathy \n",
    "\t# return \n",
    "\n",
    "\n",
    "generateCatharticResponse():\n",
    "\t# generate empathy\n",
    "\t# if stage = 5: call cognitive reframing\n",
    "\t# else questionmapper\n",
    "\n",
    "\t# build response using empathy and questions \n",
    "\n",
    "\t# return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9dbe0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9db43935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a += 5\n",
    "a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c786ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fydp1",
   "language": "python",
   "name": "fydp1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
