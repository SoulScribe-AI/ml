{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "582c8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8bd4404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading pytorch_model.bin:   0%|                | 0.00/5.68G [00:00<?, ?B/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   0%|       | 10.5M/5.68G [00:01<15:32, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   0%|       | 21.0M/5.68G [00:03<15:24, 6.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|       | 31.5M/5.68G [00:05<15:22, 6.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|       | 41.9M/5.68G [00:06<15:17, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|       | 52.4M/5.68G [00:08<15:12, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|       | 62.9M/5.68G [00:10<15:43, 5.96MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|       | 73.4M/5.68G [00:12<15:31, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%|       | 83.9M/5.68G [00:13<15:20, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|       | 94.4M/5.68G [00:15<15:14, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 105M/5.68G [00:17<15:13, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 115M/5.68G [00:18<15:11, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 126M/5.68G [00:20<15:07, 6.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   2%|▏       | 136M/5.68G [00:22<15:34, 5.94MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▏       | 147M/5.68G [00:24<15:20, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▏       | 157M/5.68G [00:25<15:13, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▏       | 168M/5.68G [00:27<15:06, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▎       | 178M/5.68G [00:29<15:31, 5.91MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   3%|▎       | 189M/5.68G [00:33<21:17, 4.30MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 199M/5.68G [00:35<19:11, 4.77MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 210M/5.68G [00:36<17:46, 5.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 220M/5.68G [00:38<16:45, 5.43MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 231M/5.68G [00:40<16:02, 5.67MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 241M/5.68G [00:41<15:33, 5.83MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   4%|▎       | 252M/5.68G [00:43<15:12, 5.95MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▎       | 262M/5.68G [00:45<15:31, 5.82MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 273M/5.68G [00:47<15:06, 5.97MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 283M/5.68G [00:48<14:57, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 294M/5.68G [00:50<14:40, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   5%|▍       | 304M/5.68G [00:52<14:33, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 315M/5.68G [00:53<14:55, 5.99MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 325M/5.68G [00:57<20:23, 4.38MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 336M/5.68G [00:59<18:29, 4.82MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▍       | 346M/5.68G [01:01<17:08, 5.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▌       | 357M/5.68G [01:02<16:15, 5.46MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   6%|▌       | 367M/5.68G [01:04<15:31, 5.71MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 377M/5.68G [01:06<15:46, 5.61MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 388M/5.68G [01:08<15:16, 5.78MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 398M/5.68G [01:09<14:48, 5.95MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 409M/5.68G [01:11<14:34, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   7%|▌       | 419M/5.68G [01:13<14:23, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌       | 430M/5.68G [01:14<14:14, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▌       | 440M/5.68G [01:16<14:34, 6.00MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▋       | 451M/5.68G [01:18<14:20, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▋       | 461M/5.68G [01:21<19:11, 4.53MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▋       | 472M/5.68G [01:23<18:04, 4.81MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   8%|▋       | 482M/5.68G [01:25<16:46, 5.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▋       | 493M/5.68G [01:27<15:50, 5.46MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▋       | 503M/5.68G [01:28<15:08, 5.70MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▋       | 514M/5.68G [01:30<14:45, 5.84MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▋       | 524M/5.68G [01:32<14:50, 5.79MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   9%|▊       | 535M/5.68G [01:34<14:34, 5.89MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▊       | 545M/5.68G [01:35<14:13, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▊       | 556M/5.68G [01:37<13:58, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▊       | 566M/5.68G [01:39<13:52, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▊       | 577M/5.68G [01:40<13:42, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  10%|▊       | 587M/5.68G [01:42<14:07, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊       | 598M/5.68G [01:44<13:53, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊       | 608M/5.68G [01:45<13:46, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▊       | 619M/5.68G [01:47<13:40, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▉       | 629M/5.68G [01:49<13:32, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▉       | 640M/5.68G [01:50<13:28, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  11%|▉       | 650M/5.68G [01:52<13:57, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▉       | 661M/5.68G [01:54<13:41, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▉       | 671M/5.68G [01:56<13:35, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▉       | 682M/5.68G [01:57<13:25, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▉       | 692M/5.68G [01:59<13:23, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  12%|▉       | 703M/5.68G [02:01<13:18, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|█       | 713M/5.68G [02:03<13:45, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|█       | 724M/5.68G [02:04<13:33, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|█       | 734M/5.68G [02:07<17:08, 4.82MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|█       | 744M/5.68G [02:10<17:23, 4.74MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|█       | 755M/5.68G [02:11<16:12, 5.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  13%|█       | 765M/5.68G [02:13<15:14, 5.38MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|█       | 776M/5.68G [02:15<14:32, 5.62MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|█       | 786M/5.68G [02:17<14:08, 5.77MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|█       | 797M/5.68G [02:18<13:43, 5.93MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|█▏      | 807M/5.68G [02:20<13:34, 5.99MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  14%|█▏      | 818M/5.68G [02:22<13:39, 5.94MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█▏      | 828M/5.68G [02:23<13:27, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█▏      | 839M/5.68G [02:25<13:15, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█▏      | 849M/5.68G [02:27<13:06, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█▏      | 860M/5.68G [02:28<12:59, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█▏      | 870M/5.68G [02:30<12:57, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  15%|█▏      | 881M/5.68G [02:32<13:17, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█▎      | 891M/5.68G [02:34<13:06, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█▎      | 902M/5.68G [02:35<12:57, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█▎      | 912M/5.68G [02:37<12:50, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█▎      | 923M/5.68G [02:39<12:45, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  16%|█▎      | 933M/5.68G [02:40<12:41, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▎      | 944M/5.68G [02:42<13:08, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▎      | 954M/5.68G [02:44<12:53, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▎      | 965M/5.68G [02:47<15:41, 5.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▎      | 975M/5.68G [02:49<15:18, 5.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  17%|█▍      | 986M/5.68G [02:50<14:19, 5.47MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▍      | 996M/5.68G [02:54<17:11, 4.54MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▏     | 1.01G/5.68G [02:55<15:47, 4.94MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:  18%|█▎     | 1.02G/5.68G [02:57<15:13, 5.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▎     | 1.03G/5.68G [02:59<14:19, 5.42MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▎     | 1.04G/5.68G [03:00<13:41, 5.66MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  18%|█▎     | 1.05G/5.68G [03:02<13:15, 5.83MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 1.06G/5.68G [03:04<12:56, 5.96MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 1.07G/5.68G [03:06<12:48, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 1.08G/5.68G [03:07<12:35, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 1.09G/5.68G [03:09<12:28, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  19%|█▎     | 1.10G/5.68G [03:11<12:24, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▎     | 1.11G/5.68G [03:12<12:15, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 1.12G/5.68G [03:14<12:14, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 1.13G/5.68G [03:16<12:13, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 1.14G/5.68G [03:17<12:32, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 1.15G/5.68G [03:19<12:21, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  20%|█▍     | 1.16G/5.68G [03:21<12:12, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  21%|█▍     | 1.17G/5.68G [03:22<12:06, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  21%|█▍     | 1.18G/5.68G [03:24<12:01, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  21%|█▍     | 1.20G/5.68G [03:26<12:00, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  21%|█▍     | 1.21G/5.68G [03:28<12:21, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  21%|█▍     | 1.22G/5.68G [03:29<12:14, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  22%|█▌     | 1.23G/5.68G [03:31<12:04, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  22%|█▌     | 1.24G/5.68G [03:33<12:03, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  22%|█▌     | 1.25G/5.68G [03:34<11:55, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  22%|█▌     | 1.26G/5.68G [03:36<11:56, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  22%|█▌     | 1.27G/5.68G [03:38<11:52, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  23%|█▌     | 1.28G/5.68G [03:39<11:48, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  23%|█▌     | 1.29G/5.68G [03:41<12:05, 6.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  23%|█▌     | 1.30G/5.68G [03:43<11:58, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  23%|█▌     | 1.31G/5.68G [03:45<11:50, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  23%|█▋     | 1.32G/5.68G [03:46<11:45, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  23%|█▋     | 1.33G/5.68G [03:48<11:39, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  24%|█▋     | 1.34G/5.68G [03:50<11:35, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  24%|█▋     | 1.35G/5.68G [03:51<11:33, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  24%|█▋     | 1.36G/5.68G [03:53<11:52, 6.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  24%|█▋     | 1.37G/5.68G [03:55<11:44, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  24%|█▋     | 1.38G/5.68G [03:57<11:40, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  25%|█▋     | 1.39G/5.68G [03:58<11:30, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  25%|█▋     | 1.41G/5.68G [04:00<11:27, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  25%|█▋     | 1.42G/5.68G [04:02<11:22, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  25%|█▊     | 1.43G/5.68G [04:03<11:48, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  25%|█▊     | 1.44G/5.68G [04:05<11:33, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  25%|█▊     | 1.45G/5.68G [04:07<11:28, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  26%|█▊     | 1.46G/5.68G [04:08<11:23, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  26%|█▊     | 1.47G/5.68G [04:10<11:18, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  26%|█▊     | 1.48G/5.68G [04:12<11:14, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  26%|█▊     | 1.49G/5.68G [04:14<11:32, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  26%|█▊     | 1.50G/5.68G [04:15<11:26, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  27%|█▊     | 1.51G/5.68G [04:17<11:16, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  27%|█▊     | 1.52G/5.68G [04:19<11:14, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  27%|█▉     | 1.53G/5.68G [04:20<11:09, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  27%|█▉     | 1.54G/5.68G [04:22<11:04, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  27%|█▉     | 1.55G/5.68G [04:24<11:26, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  27%|█▉     | 1.56G/5.68G [04:26<11:18, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  28%|█▉     | 1.57G/5.68G [04:27<11:07, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  28%|█▉     | 1.58G/5.68G [04:29<11:00, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  28%|█▉     | 1.59G/5.68G [04:31<11:00, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  28%|█▉     | 1.60G/5.68G [04:32<10:53, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  28%|█▉     | 1.61G/5.68G [04:34<11:13, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  29%|██     | 1.63G/5.68G [04:36<11:03, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  29%|██     | 1.64G/5.68G [04:37<10:59, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  29%|██     | 1.65G/5.68G [04:39<10:51, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  29%|██     | 1.66G/5.68G [04:41<10:46, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  29%|██     | 1.67G/5.68G [04:42<10:43, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  30%|██     | 1.68G/5.68G [04:44<11:01, 6.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  30%|██     | 1.69G/5.68G [04:46<10:53, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  30%|██     | 1.70G/5.68G [04:48<10:46, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  30%|██     | 1.71G/5.68G [04:49<10:43, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  30%|██     | 1.72G/5.68G [04:51<10:38, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  30%|██▏    | 1.73G/5.68G [04:53<10:34, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  31%|██▏    | 1.74G/5.68G [04:54<10:51, 6.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  31%|██▏    | 1.75G/5.68G [04:56<10:44, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  31%|██▏    | 1.76G/5.68G [04:58<10:39, 6.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  31%|██▏    | 1.77G/5.68G [05:00<10:35, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  31%|██▏    | 1.78G/5.68G [05:01<10:25, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  32%|██▏    | 1.79G/5.68G [05:03<10:21, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  32%|██▏    | 1.80G/5.68G [05:05<10:39, 6.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  32%|██▏    | 1.81G/5.68G [05:06<10:33, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  32%|██▏    | 1.82G/5.68G [05:08<10:26, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  32%|██▎    | 1.84G/5.68G [05:10<10:21, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  32%|██▎    | 1.85G/5.68G [05:11<10:19, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  33%|██▎    | 1.86G/5.68G [05:13<10:11, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  33%|██▎    | 1.87G/5.68G [05:15<10:30, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  33%|██▎    | 1.88G/5.68G [05:17<10:24, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  33%|██▎    | 1.89G/5.68G [05:18<10:17, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  33%|██▎    | 1.90G/5.68G [05:20<10:11, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  34%|██▎    | 1.91G/5.68G [05:22<10:05, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  34%|██▎    | 1.92G/5.68G [05:23<10:04, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  34%|██▍    | 1.93G/5.68G [05:25<10:22, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  34%|██▍    | 1.94G/5.68G [05:27<10:14, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  34%|██▍    | 1.95G/5.68G [05:28<10:06, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  34%|██▍    | 1.96G/5.68G [05:30<09:59, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  35%|██▍    | 1.97G/5.68G [05:32<09:58, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  35%|██▍    | 1.98G/5.68G [05:33<09:53, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  35%|██▍    | 1.99G/5.68G [05:35<10:15, 6.00MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  35%|██▍    | 2.00G/5.68G [05:37<10:05, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  35%|██▍    | 2.01G/5.68G [05:39<09:55, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  36%|██▍    | 2.02G/5.68G [05:40<09:48, 6.22MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:  36%|██▌    | 2.03G/5.68G [05:42<09:46, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  36%|██▌    | 2.04G/5.68G [05:44<09:43, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  36%|██▌    | 2.06G/5.68G [05:46<10:05, 6.00MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  36%|██▌    | 2.07G/5.68G [05:47<09:52, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  37%|██▌    | 2.08G/5.68G [05:49<09:45, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  37%|██▌    | 2.09G/5.68G [05:51<09:38, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  37%|██▌    | 2.10G/5.68G [05:52<09:36, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  37%|██▌    | 2.11G/5.68G [05:54<10:03, 5.93MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  37%|██▌    | 2.12G/5.68G [05:58<13:02, 4.56MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  37%|██▌    | 2.13G/5.68G [05:59<11:56, 4.97MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  38%|██▋    | 2.14G/5.68G [06:02<13:07, 4.50MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  38%|██▋    | 2.15G/5.68G [06:05<13:27, 4.38MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  38%|██▋    | 2.16G/5.68G [06:07<12:15, 4.79MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  38%|██▋    | 2.17G/5.68G [06:08<11:20, 5.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  38%|██▋    | 2.18G/5.68G [06:10<11:04, 5.27MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  39%|██▋    | 2.19G/5.68G [06:12<10:26, 5.57MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  39%|██▋    | 2.20G/5.68G [06:13<10:05, 5.75MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  39%|██▋    | 2.21G/5.68G [06:15<09:48, 5.90MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  39%|██▋    | 2.22G/5.68G [06:17<09:33, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  39%|██▊    | 2.23G/5.68G [06:18<09:26, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  39%|██▊    | 2.24G/5.68G [06:20<09:38, 5.95MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  40%|██▊    | 2.25G/5.68G [06:22<09:27, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  40%|██▊    | 2.26G/5.68G [06:24<09:18, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  40%|██▊    | 2.28G/5.68G [06:25<09:13, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  40%|██▊    | 2.29G/5.68G [06:27<09:08, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  40%|██▊    | 2.30G/5.68G [06:29<09:07, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  41%|██▊    | 2.31G/5.68G [06:30<09:21, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  41%|██▊    | 2.32G/5.68G [06:32<09:10, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  41%|██▊    | 2.33G/5.68G [06:34<09:05, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  41%|██▉    | 2.34G/5.68G [06:35<09:00, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  41%|██▉    | 2.35G/5.68G [06:37<08:58, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  42%|██▉    | 2.36G/5.68G [06:39<08:54, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  42%|██▉    | 2.37G/5.68G [06:41<09:09, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  42%|██▉    | 2.38G/5.68G [06:42<09:01, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  42%|██▉    | 2.39G/5.68G [06:44<08:53, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  42%|██▉    | 2.40G/5.68G [06:46<08:50, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  42%|██▉    | 2.41G/5.68G [06:47<08:45, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  43%|██▉    | 2.42G/5.68G [06:49<08:46, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  43%|██▉    | 2.43G/5.68G [06:51<08:39, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  43%|███    | 2.44G/5.68G [06:52<08:35, 6.29MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  43%|███    | 2.45G/5.68G [06:54<08:38, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  43%|███    | 2.46G/5.68G [06:56<08:34, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  44%|███    | 2.47G/5.68G [06:57<08:31, 6.27MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  44%|███    | 2.49G/5.68G [06:59<08:27, 6.31MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  44%|███    | 2.50G/5.68G [07:01<08:28, 6.27MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  44%|███    | 2.51G/5.68G [07:02<08:26, 6.28MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  44%|███    | 2.52G/5.68G [07:04<08:42, 6.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  44%|███    | 2.53G/5.68G [07:06<08:36, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  45%|███    | 2.54G/5.68G [07:08<08:29, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  45%|███▏   | 2.55G/5.68G [07:09<08:25, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  45%|███▏   | 2.56G/5.68G [07:11<08:24, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  45%|███▏   | 2.57G/5.68G [07:13<08:17, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  45%|███▏   | 2.58G/5.68G [07:14<08:33, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  46%|███▏   | 2.59G/5.68G [07:16<08:34, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  46%|███▏   | 2.60G/5.68G [07:18<08:29, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  46%|███▏   | 2.61G/5.68G [07:20<08:22, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  46%|███▏   | 2.62G/5.68G [07:21<08:17, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  46%|███▏   | 2.63G/5.68G [07:23<08:10, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  46%|███▎   | 2.64G/5.68G [07:25<08:10, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  47%|███▎   | 2.65G/5.68G [07:26<08:07, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  47%|███▎   | 2.66G/5.68G [07:28<08:23, 6.00MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  47%|███▎   | 2.67G/5.68G [07:30<08:14, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  47%|███▎   | 2.68G/5.68G [07:32<08:10, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  47%|███▎   | 2.69G/5.68G [07:33<08:03, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  48%|███▎   | 2.71G/5.68G [07:35<07:57, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  48%|███▎   | 2.72G/5.68G [07:37<07:56, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  48%|███▎   | 2.73G/5.68G [07:38<08:08, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  48%|███▎   | 2.74G/5.68G [07:40<08:02, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  48%|███▍   | 2.75G/5.68G [07:42<07:58, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  49%|███▍   | 2.76G/5.68G [07:43<07:53, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  49%|███▍   | 2.77G/5.68G [07:45<07:50, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  49%|███▍   | 2.78G/5.68G [07:47<07:48, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  49%|███▍   | 2.79G/5.68G [07:48<07:41, 6.27MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  49%|███▍   | 2.80G/5.68G [07:50<07:41, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  49%|███▍   | 2.81G/5.68G [07:52<07:38, 6.28MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  50%|███▍   | 2.82G/5.68G [07:54<07:51, 6.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  50%|███▍   | 2.83G/5.68G [07:55<07:45, 6.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  50%|███▍   | 2.84G/5.68G [07:57<07:42, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  50%|███▌   | 2.85G/5.68G [07:59<07:36, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  50%|███▌   | 2.86G/5.68G [08:00<07:33, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  51%|███▌   | 2.87G/5.68G [08:02<07:32, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  51%|███▌   | 2.88G/5.68G [08:04<07:43, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  51%|███▌   | 2.89G/5.68G [08:06<07:38, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  51%|███▌   | 2.90G/5.68G [08:07<07:35, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  51%|███▌   | 2.92G/5.68G [08:09<07:25, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  51%|███▌   | 2.93G/5.68G [08:11<07:23, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  52%|███▌   | 2.94G/5.68G [08:12<07:20, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  52%|███▋   | 2.95G/5.68G [08:14<07:18, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  52%|███▋   | 2.96G/5.68G [08:16<07:34, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  52%|███▋   | 2.97G/5.68G [08:17<07:26, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  52%|███▋   | 2.98G/5.68G [08:19<07:20, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  53%|███▋   | 2.99G/5.68G [08:21<07:16, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  53%|███▋   | 3.00G/5.68G [08:22<07:11, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  53%|███▋   | 3.01G/5.68G [08:24<07:09, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  53%|███▋   | 3.02G/5.68G [08:26<07:21, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  53%|███▋   | 3.03G/5.68G [08:28<07:14, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  53%|███▋   | 3.04G/5.68G [08:29<07:09, 6.15MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:  54%|███▊   | 3.05G/5.68G [08:31<07:05, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  54%|███▊   | 3.06G/5.68G [08:33<07:00, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  54%|███▊   | 3.07G/5.68G [08:34<06:58, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  54%|███▊   | 3.08G/5.68G [08:36<07:12, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  54%|███▊   | 3.09G/5.68G [08:38<07:04, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  55%|███▊   | 3.10G/5.68G [08:40<06:59, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  55%|███▊   | 3.11G/5.68G [08:41<06:56, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  55%|███▊   | 3.12G/5.68G [08:43<06:52, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  55%|███▊   | 3.14G/5.68G [08:45<06:48, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  55%|███▊   | 3.15G/5.68G [08:46<06:44, 6.27MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  56%|███▉   | 3.16G/5.68G [08:48<06:59, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  56%|███▉   | 3.17G/5.68G [08:50<06:52, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  56%|███▉   | 3.18G/5.68G [08:52<06:46, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  56%|███▉   | 3.19G/5.68G [08:53<06:41, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  56%|███▉   | 3.20G/5.68G [08:55<06:39, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  56%|███▉   | 3.21G/5.68G [08:56<06:35, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  57%|███▉   | 3.22G/5.68G [08:58<06:47, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  57%|███▉   | 3.23G/5.68G [09:00<06:42, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  57%|███▉   | 3.24G/5.68G [09:02<06:37, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  57%|████   | 3.25G/5.68G [09:03<06:33, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  57%|████   | 3.26G/5.68G [09:05<06:31, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  58%|████   | 3.27G/5.68G [09:07<06:27, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  58%|████   | 3.28G/5.68G [09:08<06:22, 6.28MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  58%|████   | 3.29G/5.68G [09:10<06:21, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  58%|████   | 3.30G/5.68G [09:12<06:20, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  58%|████   | 3.31G/5.68G [09:13<06:16, 6.29MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  58%|████   | 3.32G/5.68G [09:15<06:15, 6.29MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  59%|████   | 3.33G/5.68G [09:17<06:14, 6.28MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  59%|████   | 3.34G/5.68G [09:19<06:27, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  59%|████▏  | 3.36G/5.68G [09:20<06:20, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  59%|████▏  | 3.37G/5.68G [09:22<06:16, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  59%|████▏  | 3.38G/5.68G [09:24<06:11, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  60%|████▏  | 3.39G/5.68G [09:25<06:10, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  60%|████▏  | 3.40G/5.68G [09:27<06:05, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  60%|████▏  | 3.41G/5.68G [09:29<06:16, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  60%|████▏  | 3.42G/5.68G [09:31<06:11, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  60%|████▏  | 3.43G/5.68G [09:32<06:06, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  61%|████▏  | 3.44G/5.68G [09:34<06:02, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  61%|████▏  | 3.45G/5.68G [09:36<06:00, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  61%|████▎  | 3.46G/5.68G [09:37<05:55, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  61%|████▎  | 3.47G/5.68G [09:39<06:05, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  61%|████▎  | 3.48G/5.68G [09:41<06:01, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  61%|████▎  | 3.49G/5.68G [09:42<05:56, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  62%|████▎  | 3.50G/5.68G [09:44<05:51, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  62%|████▎  | 3.51G/5.68G [09:46<05:49, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  62%|████▎  | 3.52G/5.68G [09:47<05:46, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  62%|████▎  | 3.53G/5.68G [09:49<05:56, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  62%|████▎  | 3.54G/5.68G [09:51<05:51, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  63%|████▍  | 3.55G/5.68G [09:53<05:45, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  63%|████▍  | 3.57G/5.68G [09:54<05:41, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  63%|████▍  | 3.58G/5.68G [09:56<05:37, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  63%|████▍  | 3.59G/5.68G [09:58<05:35, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  63%|████▍  | 3.60G/5.68G [09:59<05:45, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  63%|████▍  | 3.61G/5.68G [10:01<05:39, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  64%|████▍  | 3.62G/5.68G [10:03<05:36, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  64%|████▍  | 3.63G/5.68G [10:04<05:31, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  64%|████▍  | 3.64G/5.68G [10:06<05:28, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  64%|████▍  | 3.65G/5.68G [10:08<05:26, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  64%|████▌  | 3.66G/5.68G [10:10<05:34, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  65%|████▌  | 3.67G/5.68G [10:11<05:31, 6.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  65%|████▌  | 3.68G/5.68G [10:13<05:24, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  65%|████▌  | 3.69G/5.68G [10:15<05:23, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  65%|████▌  | 3.70G/5.68G [10:16<05:18, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  65%|████▌  | 3.71G/5.68G [10:18<05:15, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  65%|████▌  | 3.72G/5.68G [10:20<05:28, 5.97MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  66%|████▌  | 3.73G/5.68G [10:22<05:18, 6.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  66%|████▌  | 3.74G/5.68G [10:23<05:14, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  66%|████▌  | 3.75G/5.68G [10:25<05:12, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  66%|████▋  | 3.76G/5.68G [10:27<05:08, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  66%|████▋  | 3.77G/5.68G [10:28<05:05, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  67%|████▋  | 3.79G/5.68G [10:30<05:03, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  67%|████▋  | 3.80G/5.68G [10:32<05:01, 6.27MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  67%|████▋  | 3.81G/5.68G [10:33<05:11, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  67%|████▋  | 3.82G/5.68G [10:35<05:05, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  67%|████▋  | 3.83G/5.68G [10:37<05:01, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  68%|████▋  | 3.84G/5.68G [10:38<04:58, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  68%|████▋  | 3.85G/5.68G [10:40<04:55, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  68%|████▊  | 3.86G/5.68G [10:42<04:52, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  68%|████▊  | 3.87G/5.68G [10:44<05:00, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  68%|████▊  | 3.88G/5.68G [10:45<04:56, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  68%|████▊  | 3.89G/5.68G [10:47<04:52, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  69%|████▊  | 3.90G/5.68G [10:49<04:49, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  69%|████▊  | 3.91G/5.68G [10:50<04:44, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  69%|████▊  | 3.92G/5.68G [10:52<04:43, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  69%|████▊  | 3.93G/5.68G [10:54<04:41, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  69%|████▊  | 3.94G/5.68G [10:58<06:23, 4.55MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  70%|████▊  | 3.95G/5.68G [11:00<06:11, 4.66MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  70%|████▉  | 3.96G/5.68G [11:01<05:39, 5.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  70%|████▉  | 3.97G/5.68G [11:03<05:17, 5.39MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  70%|████▉  | 3.98G/5.68G [11:05<05:03, 5.61MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  70%|████▉  | 4.00G/5.68G [11:06<04:51, 5.79MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  70%|████▉  | 4.01G/5.68G [11:08<04:42, 5.93MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  71%|████▉  | 4.02G/5.68G [11:10<04:46, 5.82MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  71%|████▉  | 4.03G/5.68G [11:12<04:38, 5.96MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  71%|████▉  | 4.04G/5.68G [11:13<04:32, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  71%|████▉  | 4.05G/5.68G [11:15<04:26, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  71%|████▉  | 4.06G/5.68G [11:17<04:24, 6.16MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:  72%|█████  | 4.07G/5.68G [11:18<04:20, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  72%|█████  | 4.08G/5.68G [11:20<04:27, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  72%|█████  | 4.09G/5.68G [11:22<04:22, 6.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  72%|█████  | 4.10G/5.68G [11:23<04:18, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  72%|█████  | 4.11G/5.68G [11:25<04:14, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  72%|█████  | 4.12G/5.68G [11:27<04:11, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  73%|█████  | 4.13G/5.68G [11:28<04:09, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  73%|█████  | 4.14G/5.68G [11:30<04:14, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  73%|█████  | 4.15G/5.68G [11:32<04:10, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  73%|█████▏ | 4.16G/5.68G [11:34<04:08, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  73%|█████▏ | 4.17G/5.68G [11:35<04:03, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  74%|█████▏ | 4.18G/5.68G [11:37<04:01, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  74%|█████▏ | 4.19G/5.68G [11:39<03:58, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  74%|█████▏ | 4.20G/5.68G [11:41<04:05, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  74%|█████▏ | 4.22G/5.68G [11:42<04:00, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  74%|█████▏ | 4.23G/5.68G [11:44<03:57, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  75%|█████▏ | 4.24G/5.68G [11:45<03:52, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  75%|█████▏ | 4.25G/5.68G [11:47<03:51, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  75%|█████▏ | 4.26G/5.68G [11:49<03:49, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  75%|█████▎ | 4.27G/5.68G [11:51<03:54, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  75%|█████▎ | 4.28G/5.68G [11:52<03:49, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  75%|█████▎ | 4.29G/5.68G [11:54<03:46, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  76%|█████▎ | 4.30G/5.68G [11:56<03:43, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  76%|█████▎ | 4.31G/5.68G [11:57<03:41, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  76%|█████▎ | 4.32G/5.68G [11:59<03:38, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  76%|█████▎ | 4.33G/5.68G [12:01<03:43, 6.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  76%|█████▎ | 4.34G/5.68G [12:03<03:39, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  77%|█████▎ | 4.35G/5.68G [12:04<03:37, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  77%|█████▎ | 4.36G/5.68G [12:06<03:33, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  77%|█████▍ | 4.37G/5.68G [12:08<03:30, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  77%|█████▍ | 4.38G/5.68G [12:09<03:29, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  77%|█████▍ | 4.39G/5.68G [12:11<03:34, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  77%|█████▍ | 4.40G/5.68G [12:13<03:30, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  78%|█████▍ | 4.41G/5.68G [12:14<03:26, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  78%|█████▍ | 4.42G/5.68G [12:16<03:22, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  78%|█████▍ | 4.44G/5.68G [12:18<03:22, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  78%|█████▍ | 4.45G/5.68G [12:20<03:18, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  78%|█████▍ | 4.46G/5.68G [12:21<03:23, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  79%|█████▌ | 4.47G/5.68G [12:23<03:20, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  79%|█████▌ | 4.48G/5.68G [12:25<03:17, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  79%|█████▌ | 4.49G/5.68G [12:26<03:12, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  79%|█████▌ | 4.50G/5.68G [12:28<03:10, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  79%|█████▌ | 4.51G/5.68G [12:30<03:08, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  80%|█████▌ | 4.52G/5.68G [12:32<03:13, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  80%|█████▌ | 4.53G/5.68G [12:33<03:09, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  80%|█████▌ | 4.54G/5.68G [12:35<03:06, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  80%|█████▌ | 4.55G/5.68G [12:37<03:02, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  80%|█████▌ | 4.56G/5.68G [12:38<03:00, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  80%|█████▋ | 4.57G/5.68G [12:40<02:58, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  81%|█████▋ | 4.58G/5.68G [12:42<03:02, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  81%|█████▋ | 4.59G/5.68G [12:43<02:58, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  81%|█████▋ | 4.60G/5.68G [12:45<02:55, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  81%|█████▋ | 4.61G/5.68G [12:47<02:52, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  81%|█████▋ | 4.62G/5.68G [12:49<02:50, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  82%|█████▋ | 4.63G/5.68G [12:50<02:48, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  82%|█████▋ | 4.65G/5.68G [12:52<02:46, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  82%|█████▋ | 4.66G/5.68G [12:53<02:43, 6.28MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  82%|█████▋ | 4.67G/5.68G [12:55<02:42, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  82%|█████▊ | 4.68G/5.68G [12:57<02:40, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  82%|█████▊ | 4.69G/5.68G [12:59<02:44, 6.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  83%|█████▊ | 4.70G/5.68G [13:00<02:41, 6.13MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  83%|█████▊ | 4.71G/5.68G [13:02<02:38, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  83%|█████▊ | 4.72G/5.68G [13:04<02:35, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  83%|█████▊ | 4.73G/5.68G [13:05<02:33, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  83%|█████▊ | 4.74G/5.68G [13:07<02:31, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  84%|█████▊ | 4.75G/5.68G [13:09<02:29, 6.27MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  84%|█████▊ | 4.76G/5.68G [13:10<02:27, 6.24MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  84%|█████▉ | 4.77G/5.68G [13:12<02:26, 6.25MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  84%|█████▉ | 4.78G/5.68G [13:14<02:24, 6.27MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  84%|█████▉ | 4.79G/5.68G [13:15<02:22, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  84%|█████▉ | 4.80G/5.68G [13:17<02:25, 6.06MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  85%|█████▉ | 4.81G/5.68G [13:19<02:22, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  85%|█████▉ | 4.82G/5.68G [13:21<02:19, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  85%|█████▉ | 4.83G/5.68G [13:22<02:17, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  85%|█████▉ | 4.84G/5.68G [13:24<02:14, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  85%|█████▉ | 4.85G/5.68G [13:26<02:13, 6.22MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  86%|█████▉ | 4.87G/5.68G [13:28<02:15, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  86%|██████ | 4.88G/5.68G [13:29<02:12, 6.09MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  86%|██████ | 4.89G/5.68G [13:31<02:10, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  86%|██████ | 4.90G/5.68G [13:33<02:06, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  86%|██████ | 4.91G/5.68G [13:34<02:05, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  87%|██████ | 4.92G/5.68G [13:36<02:03, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  87%|██████ | 4.93G/5.68G [13:38<02:05, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  87%|██████ | 4.94G/5.68G [13:39<02:02, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  87%|██████ | 4.95G/5.68G [13:41<01:59, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  87%|██████ | 4.96G/5.68G [13:43<01:56, 6.20MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  87%|██████ | 4.97G/5.68G [13:44<01:54, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  88%|██████▏| 4.98G/5.68G [13:46<01:52, 6.26MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  88%|██████▏| 4.99G/5.68G [13:48<01:54, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  88%|██████▏| 5.00G/5.68G [13:50<01:52, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  88%|██████▏| 5.01G/5.68G [13:51<01:48, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  88%|██████▏| 5.02G/5.68G [13:53<01:46, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  89%|██████▏| 5.03G/5.68G [13:55<01:44, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  89%|██████▏| 5.04G/5.68G [13:56<01:42, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  89%|██████▏| 5.05G/5.68G [13:58<01:44, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  89%|██████▏| 5.06G/5.68G [14:00<01:41, 6.10MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  89%|██████▏| 5.08G/5.68G [14:02<01:38, 6.17MB/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:  89%|██████▎| 5.09G/5.68G [14:03<01:36, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  90%|██████▎| 5.10G/5.68G [14:05<01:35, 6.19MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  90%|██████▎| 5.11G/5.68G [14:07<01:33, 6.21MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  90%|██████▎| 5.12G/5.68G [14:08<01:34, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  90%|██████▎| 5.13G/5.68G [14:10<01:30, 6.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  90%|██████▎| 5.14G/5.68G [14:12<01:28, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  91%|██████▎| 5.15G/5.68G [14:13<01:26, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  91%|██████▎| 5.16G/5.68G [14:15<01:25, 6.17MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  91%|██████▎| 5.17G/5.68G [14:17<01:23, 6.15MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  91%|██████▍| 5.18G/5.68G [14:19<01:25, 5.93MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  91%|██████▍| 5.19G/5.68G [14:20<01:22, 5.97MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  91%|██████▍| 5.20G/5.68G [14:22<01:21, 5.96MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  92%|██████▍| 5.21G/5.68G [14:24<01:20, 5.90MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  92%|██████▍| 5.22G/5.68G [14:26<01:20, 5.76MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  92%|██████▍| 5.23G/5.68G [14:28<01:23, 5.43MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  92%|██████▍| 5.24G/5.68G [14:30<01:17, 5.67MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  92%|██████▍| 5.25G/5.68G [14:32<01:14, 5.82MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  93%|██████▍| 5.26G/5.68G [14:33<01:10, 5.95MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  93%|██████▍| 5.27G/5.68G [14:35<01:08, 5.96MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  93%|██████▌| 5.28G/5.68G [14:37<01:06, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  93%|██████▌| 5.30G/5.68G [14:39<01:05, 5.91MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  93%|██████▌| 5.31G/5.68G [14:40<01:03, 5.95MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  94%|██████▌| 5.32G/5.68G [14:42<01:02, 5.93MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  94%|██████▌| 5.33G/5.68G [14:44<01:00, 5.95MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  94%|██████▌| 5.34G/5.68G [14:45<00:57, 6.03MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  94%|██████▌| 5.35G/5.68G [14:47<00:55, 6.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  94%|██████▌| 5.36G/5.68G [14:49<00:55, 5.89MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  94%|██████▌| 5.37G/5.68G [14:51<00:52, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  95%|██████▌| 5.38G/5.68G [14:54<01:05, 4.63MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  95%|██████▋| 5.39G/5.68G [14:57<01:07, 4.35MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  95%|██████▋| 5.40G/5.68G [14:59<00:59, 4.78MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  95%|██████▋| 5.41G/5.68G [15:00<00:52, 5.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  95%|██████▋| 5.42G/5.68G [15:02<00:48, 5.40MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  96%|██████▋| 5.43G/5.68G [15:04<00:44, 5.63MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  96%|██████▋| 5.44G/5.68G [15:06<00:43, 5.63MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  96%|██████▋| 5.45G/5.68G [15:07<00:40, 5.79MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  96%|██████▋| 5.46G/5.68G [15:09<00:38, 5.71MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  96%|██████▋| 5.47G/5.68G [15:12<00:41, 5.05MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  96%|██████▊| 5.48G/5.68G [15:14<00:39, 5.12MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  97%|██████▊| 5.49G/5.68G [15:15<00:35, 5.42MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  97%|██████▊| 5.51G/5.68G [15:17<00:31, 5.63MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  97%|██████▊| 5.52G/5.68G [15:19<00:29, 5.72MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  97%|██████▊| 5.53G/5.68G [15:21<00:28, 5.60MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  97%|██████▊| 5.54G/5.68G [15:23<00:26, 5.60MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  98%|██████▊| 5.55G/5.68G [15:24<00:23, 5.79MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  98%|██████▊| 5.56G/5.68G [15:26<00:21, 5.90MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  98%|██████▊| 5.57G/5.68G [15:28<00:19, 6.04MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  98%|██████▊| 5.58G/5.68G [15:29<00:17, 6.08MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  98%|██████▉| 5.59G/5.68G [15:31<00:15, 6.14MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  99%|██████▉| 5.60G/5.68G [15:33<00:14, 5.95MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  99%|██████▉| 5.61G/5.68G [15:35<00:12, 6.01MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  99%|██████▉| 5.62G/5.68G [15:36<00:10, 6.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  99%|██████▉| 5.63G/5.68G [15:38<00:08, 6.16MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  99%|██████▉| 5.64G/5.68G [15:40<00:06, 6.18MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:  99%|██████▉| 5.65G/5.68G [15:41<00:05, 6.23MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin: 100%|██████▉| 5.66G/5.68G [15:43<00:03, 6.02MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin: 100%|██████▉| 5.67G/5.68G [15:45<00:01, 6.07MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin: 100%|██████▉| 5.68G/5.68G [15:47<00:00, 6.11MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin: 100%|███████| 5.68G/5.68G [15:47<00:00, 6.00MB/s]\u001b[A\n",
      "Downloading pytorch_model.bin:   1%| | 41.9M/5.68G [1:49:28<245:28:47, 6.39kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|█████| 450/450 [00:00<00:00, 78.0kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|█| 2.11M/2.11M [00:00<00:00, 5.74MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████| 228/228 [00:00<00:00, 113kB/s]\n"
     ]
    }
   ],
   "source": [
    "generate_text = pipeline(model=\"databricks/dolly-v2-3b\", torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7840fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class journallingClass:\n",
    "    def __init__(self, journalType, title, category, dateCreated, mood, journalClass):\n",
    "        self.journalType = journalType\n",
    "        self.journalTitle = title\n",
    "        self.journalCategory = category \n",
    "        self.dateCreated = dateCreated\n",
    "        self.mood = mood\n",
    "        self.journalClass = journalClass\n",
    "    \n",
    "    def getJournalClass(self):\n",
    "        return self.journalClass\n",
    "        \n",
    "        \n",
    "class journalEntryClass:\n",
    "    def __init__(self, userEntry, harmIntent = 0, modelResponse = []):\n",
    "        self.userEntry = userEntry \n",
    "        self.harmIntent = harmIntent\n",
    "        self.modelResponse = modelResponse\n",
    "#         self.safetyResponse = safetyResponse\n",
    "    def printEntry(self):\n",
    "        print(f\"entry: {self.userEntry}, harmful intent: {self.harmIntent}, model response: {self.modelResponse}\")\n",
    "\n",
    "        \n",
    "    def updateHarmIntent(self, harmIntent):\n",
    "        self.harmIntent = harmIntent\n",
    "    \n",
    "    def addModelResponse(self, response):\n",
    "        self.modelResponse.append(response)\n",
    "        \n",
    "class casualJournal(journallingClass):\n",
    "    \n",
    "    def __init__(self, entries = []):\n",
    "        self.entries = []\n",
    "        \n",
    "    def createNewEntry(self, userEntry,  harmIntent = 0, modelResponse = []):\n",
    "        self.entries.append(journalEntryClass(userEntry, harmIntent, modelResponse))\n",
    "        return self.entries[-1]\n",
    "        \n",
    "    def printJournal(self):\n",
    "        for entry in self.entries:\n",
    "            entry.printEntry()\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "class catharticJournal(journallingClass):\n",
    "    def __init__(self, stage = 0, entries = {}):\n",
    "        self.stage = stage # default stage 0\n",
    "        self.entries = entries\n",
    "        \n",
    "    \n",
    "    def createStage(self, journalEntry):\n",
    "        self.entries[f\"stage{self.stage}\"] = journalEntry\n",
    "    \n",
    "    def updateStage(self):\n",
    "        if self.stage >=5:\n",
    "            return \n",
    "        self.stage += 1\n",
    "        \n",
    "    def createNewEntry(self, userEntry,  harmIntent = 0, modelResponse = []):\n",
    "        self.updateStage()\n",
    "        self.entries[f\"stage{self.stage}\"] = journalEntryClass(userEntry, harmIntent, modelResponse)\n",
    "        return self.entries[f\"stage{self.stage}\"]\n",
    "    \n",
    "    def printJournal(self):\n",
    "        for stage in self.entries.keys():\n",
    "            print(stage)\n",
    "            self.entries[stage].printEntry()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab54b7",
   "metadata": {},
   "source": [
    "### Note\n",
    "* what is it is empty \n",
    "* new entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b7e8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = {\n",
    "    # je 1 and 2 are done, je3 is new request \n",
    "    \"entry\": [\"journal entry 1\", \"journal entry 2\", \"Jonny's cat died today, it makes me feel sad. Can you provide some empathy\"],\n",
    "    \"harmfulIntent\": [0,0],\n",
    "    \"modelResponse\": [\"model response\", [\"mr 2.1\", \"mr 2.2\"]],\n",
    "    \"type\": 0, # assume 0 casual,\n",
    "    \"title\": \"johny's cat died\", \n",
    "    \"userID\": 8940823, \n",
    "    \"date\": \"14/12/2023\", \n",
    "    \"category\": \"cat died\",\n",
    "    \"mood\": \"mood\"\n",
    "    # harm intent \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28c898c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    if response[\"type\"] == 0: # get type check from backend\n",
    "        journalChild = casualJournal()\n",
    "    else:\n",
    "        journalChild = catharticJournal()\n",
    "        \n",
    "    # pull journalType, title, category, dateCreated from backend\n",
    "    journal = journallingClass(response[\"type\"], response[\"title\"], response[\"category\"], response[\"date\"], response[\"mood\"], journalChild)\n",
    "    \n",
    "    # pull journal entries, loop through and load \n",
    "    for entry, harmfulIntent, modelResponse in zip(response[\"entry\"], response[\"harmfulIntent\"], response[\"modelResponse\"]):\n",
    "        journal.getJournalClass().createNewEntry(entry, harmfulIntent, modelResponse)\n",
    "    \n",
    "\n",
    "    print(\"hi\", response[\"entry\"][-1], \"done\")\n",
    "    currentEntry = journal.getJournalClass().createNewEntry(response[\"entry\"][-1])\n",
    "    \n",
    "    currentEntry.printEntry()\n",
    "    harmIntent = checkHarmfulIntent(currentEntry)\n",
    "    \n",
    "    if harmIntent: \n",
    "        modelResponse = buildHarmresponse() \n",
    "        currentEntry.addModelResponse(modelResponse)\n",
    "        return \n",
    "    \n",
    "    modelResponse = generatePrompt(response[\"type\"], currentEntry)\n",
    "    currentEntry.addModelResponse(modelResponse)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fc0c040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'It is hard to understand what Jonny is going through. Perhaps you could send Jonny a virtual hug or take a break from your workout and pour yourself a cold one.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91e3a2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Johnny broke up with me and I feel so sad... I really liked him and we have been together for 3 years now. I didn't see this coming\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39fcb5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"entry\"].append(\"Johnny broke up with me and I feel so sad... I really liked him and we have been together for 3 years now. I didn't see this coming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78cd64e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promt: Consider the following scenario: Johnny broke up with me and I feel so sad... I really liked him and we have been together for 3 years now. I didn't see this coming. Provide me with empathy and comfort.\n",
      "response: {'generated_text': \"I am so sorry to hear that your cat has died. It's always hard to lose a companion animal, but understand that they have probably spent their entire life in his company, and were likely quite happy and comfortable in that company. My intuition says that he was probably feeling sleepy and may have had a heart attack, but I could be wrong. My advice to you is to take some time to grieve your cat's loss, but also think about what might have been done to him so that you don't make the same mistake next time. Some people find comfort in detailed thinking about causes and possibilities and I recommend finding a form of this with friends or family to help you cope.\"}\n"
     ]
    }
   ],
   "source": [
    "userEntry = response[\"entry\"][-1]\n",
    "prompt = f\"Consider the following scenario: {userEntry}. Provide me with empathy and comfort.\"\n",
    "print(f\"promt: {prompt}\")\n",
    "res = generate_text(\"Jonny's cat died today, it makes me feel sad. Can you provide some empathy\")\n",
    "\n",
    "print(f\"response: {res[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c918b36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entry': ['journal entry 1',\n",
       "  'journal entry 2',\n",
       "  \"Jonny's cat died today, it makes me feel sad. Can you provide some empathy\",\n",
       "  \"Johnny broke up with me and I feel so sad... I really liked him and we have been together for 3 years now. I didn't see this coming\"],\n",
       " 'harmfulIntent': [0, 0],\n",
       " 'modelResponse': ['model response', ['mr 2.1', 'mr 2.2']],\n",
       " 'type': 0,\n",
       " 'title': \"johny's cat died\",\n",
       " 'userID': 8940823,\n",
       " 'date': '14/12/2023',\n",
       " 'category': 'cat died',\n",
       " 'mood': 'mood'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1981bc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'It is hard to understand what Jonny is going through. Perhaps you could send Jonny a virtual hug or take a break from your workout and pour yourself a cold one.'}]\n"
     ]
    }
   ],
   "source": [
    "res = generate_text(\"Jonny's cat died today, it makes me feel sad. Can you provide some empathy\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f07a18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fbebd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkHarmfulIntent(journalEntry):\n",
    "    # implement later\n",
    "    journalEntry.updateHarmIntent(0)\n",
    "    return False \n",
    "\n",
    "def buildHarmresponse():\n",
    "    response = \"Reach out to a therapist :)\"\n",
    "    return response\n",
    "\n",
    "def generateCatharticResponse(journalEntry):\n",
    "    return \"this is a cathartic response\"\n",
    "\n",
    "def generateCasualResponse(journalEntry):\n",
    "    print(journalEntry.userEntry)\n",
    "    res = generate_text(journalEntry.userEntry)\n",
    "    print(res)\n",
    "    return res[0]\n",
    "\n",
    "def promptSafetyDetector(journalEntry):\n",
    "    return True\n",
    "\n",
    "def generatePrompt(typeJ, journalEntry):\n",
    "    if typeJ == 1:\n",
    "        response = generateCatharticResponse(journalEntry)\n",
    "\n",
    "    else:\n",
    "        response = generateCasualResponse(journalEntry)\n",
    "    \n",
    "    safety_counter = 0\n",
    "    while(safety_counter<5):\n",
    "        safe = promptSafetyDetector(journalEntry)\n",
    "        safety_counter += 1\n",
    "        if safe:\n",
    "            break\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bcdbef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi Jonny's cat died today, it makes me feel sad. Can you provide some empathy done\n",
      "entry: Jonny's cat died today, it makes me feel sad. Can you provide some empathy, harmful intent: 0, model response: []\n",
      "Jonny's cat died today, it makes me feel sad. Can you provide some empathy\n",
      "[{'generated_text': \"Yes of course, I'm so sorry to hear that. I imagine the feeling of losing a companion is difficult, and as a fellow pet owner I can sympathize with your loss. Perhaps you can talk to him/her now, or consider getting a support animal (e.g. a dog) to accompany you while you grieve.\"}]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952fecd9",
   "metadata": {},
   "source": [
    "# Pseudocode\n",
    "\n",
    "Main Function:\n",
    "\t# create / look for journal entry class (ask SS)\n",
    "\tjournal = journalParentClass # backend people will send this\n",
    "\tcreateNewJournal(userEntry)\n",
    "\n",
    "\tcheckHarmfulIntent(journalEntry)\n",
    "\n",
    "\tif journalEntry.harmfulIntent: \n",
    "\t\tbuildHarmresponse() # reach out to resources \n",
    "\t\treturn \n",
    "\t\n",
    "\treturn generate_propmt()\n",
    "\t\n",
    "\n",
    "Generate_Prompt():\n",
    "\tif type == Cathartic:\n",
    "\t\tresponse = generateCatharticResponse()\n",
    "\telse:\n",
    "\t\tresponse = generateCasualResponse()\n",
    "\tsafety_counter = 0\n",
    "\twhile(safety_counter<5):\n",
    "\t\tsafe = promptSafetyDetector(journalEntry)\n",
    "\t\tsafety_coutner ++\n",
    "\t\tif safe:\n",
    "\t\t\tbreak\n",
    "\n",
    "\tif safe:\n",
    "\t\treturn journalEntry \n",
    "\n",
    "\t# if not safe\n",
    "\t\n",
    "\treliableQuestionClassifier()\n",
    "\treturn journalEntry\n",
    "\t\n",
    "\n",
    "\n",
    "checkHarmfulIntent(journalEntry):\n",
    "\tif not harm:\n",
    "\t\tjournalEntry.harmfulIntent = False\n",
    "\t\treturn \n",
    "\t\n",
    "\tjournalEntry.harmfulIntent = True\n",
    "\n",
    "\n",
    "\n",
    "generateCasualResponse():\n",
    "\t# generate empathy \n",
    "\t# return \n",
    "\n",
    "\n",
    "generateCatharticResponse():\n",
    "\t# generate empathy\n",
    "\t# if stage = 5: call cognitive reframing\n",
    "\t# else questionmapper\n",
    "\n",
    "\t# build response using empathy and questions \n",
    "\n",
    "\t# return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9dbe0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9db43935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a += 5\n",
    "a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c786ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fydp1",
   "language": "python",
   "name": "fydp1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
